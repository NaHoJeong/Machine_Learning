# Dense(x, input_dim = y, activation = 'relu') -> 출력 뉴런(노드)의 수 : x, 입력 뉴런의 수 : y, 활성함수 : relu
  # input_dim은 input layer에서만 선언, hidden layer에선 그냥 Dense(x) -> 노드의 개수는 x, output layer에선 x = 1, 활성함수만 변경
  # 이진 분류일 경우 loss=binary_crossentropy, 다중 분류일 경우 loss=categorical_crossentropy 사용
# CNN의 경우, Conv2D(x, (y,y), padding='valid', input_shape=(a,a,b), activation='c')
  # -> 필터의 수 : x, 필터의 사이즈 : (y,y), padding 여부, input의 사이즈(?) : (a,a) 흑백은 b=1 RGB는 b=3
  # 2D 이미지를 입력으로 하는 맨 마지막 layer 이 후, Flatten을 통해 1차원 데이터로 변경 후 neural network 진행
    # 활성함수 부분의 default는 lenear로 계산된 결과가 그대로 출력, relu는 rectifier로 주로 hidden layer
    # sigmoid는 이진 분류 함수로 주로 output layer, softmax는 다중 클래스 분류로 주로 output layer)
    
# Neural Network를 활용한 x+y 학습 예제

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import pandas as pd
import numpy as np
import tensorflow as tf

// 데이터 파일 가져오기
file_path = 'D:Owner/documents/ML_ex/neural_network_ex.csv'
data = pd.read_csv(file_path)

x_len = len(data.columns) - 1
y_len = len(data.columns) - 1

// 가져온 데이터에서 입력 데이터는 0~ 마지막-1 번째 컬럼, 출력 데이터는 맨 마지막 컬럼
input_data = data.iloc[:, 0:x_len]
output_data = data.iloc[:, y_len]

// 학습 데이터와 테스트 데이터 분할
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=0)

// 데이터 전처리 과정
# Feature Scaling
#from sklearn.preprocessing import StandardScaler
#sc = StandardScaler()
#_train = sc.fit_transform(x_train)
#x_test = sc.transform(x_test)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# 학습 모델 만들기
NN = Sequential()
NN.add(Dense(2, input_dim=2, activation='relu')) # 첫 번째 layer(input layer), 입력 2개, layer의 노드 수 2개, 활성 함수 relu
#NN.add(Dense(2, activation='relu')) # 두 번째 layer, 노드 2개, 활성 함수 relu
NN.add(Dense(1, activation='linear')) # 세 번째 layer(output layer), 노드 1개(결과 값), 활성 함수 linear(결과 그대로, 기본 값)

NN.summary()

NN.compile(loss='mse',optimizer='adam') # 손실 함수는 mse(회귀 문제), optimizer는 adam
NN.fit(x_train, y_train, epochs=200, batch_size=5) # epoch은 200, batch 사이즈는 5
  # epoch는 전체 데이터 셋에 대해서 한 번 학습을 완료한 상태 --> 200번 학습하겠다는 의미
  # batch 사이즈는 하나의 데이터에 대해 학습 한 뒤 가중치를 갱신하지 않고 batch_size 만큼 한번에 계산한 뒤 가중치 계산
loss = NN.evaluate(x_test, y_test)

y_pred = NN.predict(x_test) # test 데이터에 대한 결과
print(y_test, y_pred) # 실제 결과와 예측된 결과 출력

x = np.array([[10,9]]) # 새로운 데이터 입력
p=NN.predict(x)
print(p)

# 학습 모델 저장
from tensorflow.keras.models import load_model
NN.save('NN_ex1.hs')
